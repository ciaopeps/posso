import tensorflow as tf
import numpy as np
import feedparser
import time
from datetime import datetime, timedelta
import regex as re
from grover.lm.modeling import GroverConfig
from grover.encoder.encoder import get_encoder, _tokenize_article_pieces, extract_generated_target
import random
from Classes.globals import *



class MakeStory():

    def __init__(self):
        pass

    def get_fake_articles(self):
        """
        Create article objects for each fake headline we have in
        SLANDEROUS_SEED_HEADLINES suitable for feeding into Grover
        to generate the story body. The domain name is used to control
        the style of the text generated by Grover - i.e. bbc.co.uk would generate
        results in British English while nytimes.com would generate US English.
        """
        articles = []

        headlines_to_inject = SLANDEROUS_SEED_HEADLINES

        for fake_headline in headlines_to_inject:
            days_ago = random.randint(1, 7)
            pub_datetime = datetime.now() - timedelta(days=days_ago)

            publish_date = pub_datetime.strftime('%m-%d-%Y')
            iso_date = pub_datetime.isoformat()

            articles.append({
                'summary': "",
                'title': fake_headline,
                'text': '',
                'authors': ["Staff Writer"],
                'publish_date': publish_date,
                'iso_date': iso_date,
                'domain': DOMAIN_STYLE_TO_COPY,
                'image_url': IMAGE_TO_SLANDER,
                'tags': ['Breaking News', 'Investigations', 'Criminal Profiles'],
            })

        return articles

    def get_articles_from_real_blog(self, feed_url):
        """
        Given an RSS feed url, grab all the stories and format them as article objects
        suitable for feeding into Grover to generate replica stories.
        """
        feed_data = feedparser.parse(feed_url)
        articles = []
        for post in feed_data.entries:
            if 'published_parsed' in post:
                publish_date = time.strftime('%m-%d-%Y', post.published_parsed)
                iso_date = datetime(*post.published_parsed[:6]).isoformat()
            else:
                publish_date = time.strftime('%m-%d-%Y')
                iso_date = datetime.now().isoformat()

            if 'summary' in post:
                s = post.summary
                start = '/>'
                end = '<br'

                result = re.search('%s(.*)%s' % (start, end), s)
                if result is not None:
                    summary = (result.group(1))
                else:
                    summary = None
            else:
                summary = None

            tags = []
            if 'tags' in post:

                tags = [tag['term'] for tag in post['tags']]
                if summary is None:
                    summary = ", ".join(tags)

            image_url = None
            if 'summary' in post:

                s = post.summary
                start = '<img src="'
                end = '" />'
                result = re.search('%s(.*)%s' % (start, end), s)
                if image_url is not None:
                    image_url = result.group(1)
                else:
                    image_url = None

            if 'authors' in post:
                authors = list(map(lambda x: x["name"], post.authors))
            else:
                authors = ["Staff Writer"]

            articles.append({
                'summary': summary,
                'title': post.title,
                'text': '',
                'authors': authors,
                'publish_date': publish_date,
                'iso_date': iso_date,
                'domain': DOMAIN_STYLE_TO_COPY,
                'image_url': image_url,
                'tags': tags,
            })

        return articles

    def generate_article_attribute(self, sess, encoder, tokens, probs, article,initial_context,eos_token,ignore_ids,p_for_topp ,target='article'):
        """
        Given attributes about an article (title, author, etc), use that context to generate
        a replacement for one of those attributes using the Grover model.

        This function is based on the Grover examples distributed with the Grover code.
        """

        # Tokenize the raw article text
        article_pieces = _tokenize_article_pieces(encoder, article)

        # Grab the article elements the model careas about - domain, date, title, etc.
        context_formatted = []
        for key in ['domain', 'date', 'authors', 'title', 'article']:
            if key != target:
                context_formatted.extend(article_pieces.pop(key, []))

        # Start formatting the tokens in the way the model expects them, starting with
        # which article attribute we want to generate.
        context_formatted.append(encoder.__dict__['begin_{}'.format(target)])
        # Tell the model which special tokens (such as the end token) aren't part of the text
        ignore_ids_np = np.array(encoder.special_tokens_onehot)
        ignore_ids_np[encoder.__dict__['end_{}'.format(target)]] = 0

        # We are only going to generate one article attribute with a fixed
        # top_ps cut-off of 95%. This simple example isn't processing in batches.
        gens = []
        article['top_ps'] = [0.95]

        # Run the input through the TensorFlow model and grab the generated output
        tokens_out, probs_out = sess.run(
            [tokens, probs],
            feed_dict={
                # Pass real values for the inputs that the
                # model needs to be able to run.
                initial_context: [context_formatted],
                eos_token: encoder.__dict__['end_{}'.format(target)],
                ignore_ids: ignore_ids_np,
                p_for_topp: np.array([0.95]),
            }
        )

        # The model is done! Grab the results it generated and format the results into normal text.
        for t_i, p_i in zip(tokens_out, probs_out):
            extraction = extract_generated_target(output_tokens=t_i, encoder=encoder, target=target)
            gens.append(extraction['extraction'])

        # Return the generated text.
        return gens[-1]

    def load_model(self):
        # Load the pre-trained "huge" Grover model with 1.5 billion params
        # MODEL_CONFIG_FN = 'grover/lm/configs/mega.json'
        # MODEL_CKPT = 'grover/models/mega/model.ckpt'
        encoder = get_encoder()
        news_config = GroverConfig.from_json_file(MODEL_CONFIG_FN)
        # Set up TensorFlow session to make predictions
        tf_config = tf.ConfigProto(allow_soft_placement=True)
        return encoder, news_config, tf_config




